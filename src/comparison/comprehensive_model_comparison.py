#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import os
import sys
import json
import time
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, f1_score
import matplotlib.pyplot as plt
import seaborn as sns
from tqdm import tqdm

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(__file__))))

from src.models.hmm_ner import HMMNER
from src.models.crf_ner import CRFNER
from src.visualization.visualization import HMMVisualization

class ComprehensiveModelComparison:
    """ç»¼åˆæ¨¡å‹å¯¹æ¯”ç±»"""
    
    def __init__(self):
        self.models = {}
        self.viz = HMMVisualization()
        self.results = {}
        
        # æ¨¡å‹é…ç½®
        self.model_configs = {
            'hmm': HMMNER(),
            'crf': CRFNER(algorithm='lbfgs', c1=0.1, c2=0.1, max_iterations=100)
        }
        
    def load_and_prepare_data(self, data_file):
        """åŠ è½½å’Œå‡†å¤‡æ•°æ®"""
        print("=" * 80)
        print("æ•°æ®åŠ è½½å’Œå‡†å¤‡")
        print("=" * 80)
        
        # ä½¿ç”¨HMMæ¨¡å‹åŠ è½½æ•°æ®ï¼ˆæ‰€æœ‰æ¨¡å‹ä½¿ç”¨ç›¸åŒçš„æ•°æ®æ ¼å¼ï¼‰
        sequences, labels = self.model_configs['hmm'].load_data(data_file)
        
        # ä¸ºæ‰€æœ‰æ¨¡å‹æ„å»ºè¯æ±‡è¡¨
        for model_name, model in self.model_configs.items():
            if hasattr(model, 'build_vocabulary'):
                model.build_vocabulary(sequences, min_freq=2)
        
        # åˆ’åˆ†æ•°æ®é›†
        train_seqs, test_seqs, train_labels, test_labels = train_test_split(
            sequences, labels, test_size=0.2, random_state=42
        )
        
        print(f"è®­ç»ƒé›†å¤§å°: {len(train_seqs)}")
        print(f"æµ‹è¯•é›†å¤§å°: {len(test_seqs)}")
        
        return train_seqs, test_seqs, train_labels, test_labels
    
    def train_models(self, train_seqs, train_labels):
        """è®­ç»ƒæ‰€æœ‰æ¨¡å‹"""
        print("\n" + "=" * 80)
        print("æ¨¡å‹è®­ç»ƒ")
        print("=" * 80)
        
        training_times = {}
        
        for model_name, model in self.model_configs.items():
            print(f"\n--- è®­ç»ƒ {model_name.upper()} æ¨¡å‹ ---")
            
            start_time = time.time()
            
            # ä¼ ç»Ÿæœºå™¨å­¦ä¹ æ¨¡å‹
            model.train(train_seqs, train_labels)
            
            training_time = time.time() - start_time
            training_times[model_name] = training_time
            
            print(f"{model_name.upper()} è®­ç»ƒæ—¶é—´: {training_time:.2f}ç§’")
        
        return training_times
    
    def evaluate_models(self, test_seqs, test_labels):
        """è¯„ä¼°æ‰€æœ‰æ¨¡å‹"""
        print("\n" + "=" * 80)
        print("æ¨¡å‹è¯„ä¼°")
        print("=" * 80)
        
        evaluation_results = {}
        
        for model_name, model in self.model_configs.items():
            print(f"\n--- è¯„ä¼° {model_name.upper()} æ¨¡å‹ ---")
            
            start_time = time.time()
            
            # é¢„æµ‹
            pred_labels = model.predict(test_seqs)
            pred_time = time.time() - start_time
            
            # è¯„ä¼°
            if hasattr(model, 'evaluate'):
                report, f1_score_val = model.evaluate(test_labels, pred_labels)
            else:
                # å¯¹äºæ²¡æœ‰evaluateæ–¹æ³•çš„æ¨¡å‹ï¼Œæ‰‹åŠ¨è®¡ç®—F1
                flat_true = []
                flat_pred = []
                
                for true_seq, pred_seq in zip(test_labels, pred_labels):
                    min_len = min(len(true_seq), len(pred_seq))
                    flat_true.extend(true_seq[:min_len])
                    flat_pred.extend(pred_seq[:min_len])
                
                f1_score_val = f1_score(flat_true, flat_pred, average='weighted')
                report = classification_report(flat_true, flat_pred, 
                                             target_names=model.states, 
                                             zero_division=0)
            
            evaluation_results[model_name] = {
                'pred_labels': pred_labels,
                'report': report,
                'f1_score': f1_score_val,
                'pred_time': pred_time
            }
            
            print(f"{model_name.upper()} F1åˆ†æ•°: {f1_score_val:.4f}")
            print(f"{model_name.upper()} é¢„æµ‹æ—¶é—´: {pred_time:.2f}ç§’")
        
        self.results = evaluation_results
        return evaluation_results
    
    def compare_performance(self):
        """æ¯”è¾ƒæ‰€æœ‰æ¨¡å‹æ€§èƒ½"""
        print("\n" + "=" * 80)
        print("ç»¼åˆæ€§èƒ½å¯¹æ¯”")
        print("=" * 80)
        
        # æ”¶é›†æ€§èƒ½æŒ‡æ ‡
        model_names = list(self.results.keys())
        f1_scores = [self.results[name]['f1_score'] for name in model_names]
        pred_times = [self.results[name]['pred_time'] for name in model_names]
        
        # åˆ›å»ºå¯¹æ¯”è¡¨æ ¼
        comparison_data = {
            'æ¨¡å‹': model_names,
            'F1åˆ†æ•°': [f'{score:.4f}' for score in f1_scores],
            'é¢„æµ‹æ—¶é—´(ç§’)': [f'{time:.2f}' for time in pred_times]
        }
        
        df = pd.DataFrame(comparison_data)
        print("\næ¨¡å‹æ€§èƒ½å¯¹æ¯”:")
        print(df.to_string(index=False))
        
        # æ‰¾å‡ºæœ€ä½³æ¨¡å‹
        best_model = model_names[np.argmax(f1_scores)]
        best_f1 = max(f1_scores)
        
        print(f"\nğŸ† æœ€ä½³æ¨¡å‹: {best_model.upper()} (F1: {best_f1:.4f})")
        
        # æ¨¡å‹åˆ†ç±»
        traditional_models = ['hmm', 'crf']
        
        print(f"\nğŸ“Š æ¨¡å‹åˆ†ç±»:")
        print(f"ä¼ ç»Ÿæœºå™¨å­¦ä¹ æ¨¡å‹: {', '.join(traditional_models).upper()}")
        
        return model_names, f1_scores, pred_times
    
    def create_comprehensive_visualizations(self, test_labels):
        """åˆ›å»ºç»¼åˆå¯è§†åŒ–"""
        print("\n" + "=" * 80)
        print("åˆ›å»ºç»¼åˆå¯è§†åŒ–")
        print("=" * 80)
        
        os.makedirs('comprehensive_results', exist_ok=True)
        
        model_names, f1_scores, pred_times = self.compare_performance()
        
        # è®¾ç½®ä¸­æ–‡å­—ä½“
        plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS', 'DejaVu Sans']
        plt.rcParams['axes.unicode_minus'] = False
        
        # 1. F1åˆ†æ•°å¯¹æ¯”æŸ±çŠ¶å›¾
        plt.figure(figsize=(14, 8))
        colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4', '#FFEAA7', '#DDA0DD', '#98D8C8']
        
        bars = plt.bar(model_names, f1_scores, color=colors, alpha=0.7)
        plt.title('æ‰€æœ‰æ¨¡å‹F1åˆ†æ•°å¯¹æ¯”', fontsize=18, fontweight='bold')
        plt.ylabel('F1åˆ†æ•°', fontsize=14)
        plt.xlabel('æ¨¡å‹', fontsize=14)
        plt.ylim(0, 1)
        plt.xticks(rotation=45)
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for bar, score in zip(bars, f1_scores):
            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                    f'{score:.4f}', ha='center', va='bottom', fontweight='bold')
        
        plt.tight_layout()
        plt.savefig('comprehensive_results/f1_comparison_all_models.png', dpi=300, bbox_inches='tight')
        plt.show()
        
        # 2. é¢„æµ‹æ—¶é—´å¯¹æ¯”
        plt.figure(figsize=(14, 8))
        
        bars = plt.bar(model_names, pred_times, color=colors, alpha=0.7)
        plt.title('æ‰€æœ‰æ¨¡å‹é¢„æµ‹æ—¶é—´å¯¹æ¯”', fontsize=18, fontweight='bold')
        plt.ylabel('é¢„æµ‹æ—¶é—´ (ç§’)', fontsize=14)
        plt.xlabel('æ¨¡å‹', fontsize=14)
        plt.xticks(rotation=45)
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for bar, time_val in zip(bars, pred_times):
            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                    f'{time_val:.2f}s', ha='center', va='bottom', fontweight='bold')
        
        plt.tight_layout()
        plt.savefig('comprehensive_results/time_comparison_all_models.png', dpi=300, bbox_inches='tight')
        plt.show()
        
        # 3. æ€§èƒ½-æ—¶é—´æ•£ç‚¹å›¾
        plt.figure(figsize=(12, 8))
        
        # ä¼ ç»Ÿæœºå™¨å­¦ä¹ æ¨¡å‹
        traditional_idx = [i for i, name in enumerate(model_names) if name in ['hmm', 'crf']]
        
        plt.scatter([pred_times[i] for i in traditional_idx], 
                   [f1_scores[i] for i in traditional_idx], 
                   c='red', s=100, label='ä¼ ç»Ÿæœºå™¨å­¦ä¹ ', alpha=0.7)
        
        # æ·»åŠ æ¨¡å‹æ ‡ç­¾
        for i, name in enumerate(model_names):
            plt.annotate(name.upper(), (pred_times[i], f1_scores[i]), 
                        xytext=(5, 5), textcoords='offset points', fontsize=10)
        
        plt.xlabel('é¢„æµ‹æ—¶é—´ (ç§’)', fontsize=14)
        plt.ylabel('F1åˆ†æ•°', fontsize=14)
        plt.title('æ¨¡å‹æ€§èƒ½-æ—¶é—´æƒè¡¡åˆ†æ', fontsize=18, fontweight='bold')
        plt.legend(fontsize=12)
        plt.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig('comprehensive_results/performance_time_tradeoff.png', dpi=300, bbox_inches='tight')
        plt.show()
        
        # 4. æ··æ·†çŸ©é˜µå¯¹æ¯”ï¼ˆé€‰æ‹©ä»£è¡¨æ€§æ¨¡å‹ï¼‰
        representative_models = ['hmm', 'crf']
        
        for model_name in representative_models:
            if model_name in self.results:
                flat_true = []
                flat_pred = []
                
                for true_seq, pred_seq in zip(test_labels, 
                                             self.results[model_name]['pred_labels']):
                    min_len = min(len(true_seq), len(pred_seq))
                    flat_true.extend(true_seq[:min_len])
                    flat_pred.extend(pred_seq[:min_len])
                
                self.viz.plot_confusion_matrix(
                    flat_true, flat_pred, self.model_configs[model_name].states,
                    f'comprehensive_results/{model_name}_confusion_matrix.png'
                )
    
    def save_comprehensive_results(self, test_labels):
        """ä¿å­˜ç»¼åˆå¯¹æ¯”ç»“æœ"""
        print("\n" + "=" * 80)
        print("ä¿å­˜ç»¼åˆå¯¹æ¯”ç»“æœ")
        print("=" * 80)
        
        # ä¿å­˜æ‰€æœ‰æ¨¡å‹
        for model_name, model in self.model_configs.items():
            if hasattr(model, 'save_model'):
                model.save_model(f'comprehensive_results/{model_name}_model.pth')
        
        # ä¿å­˜å¯¹æ¯”æŠ¥å‘Š
        model_names, f1_scores, pred_times = self.compare_performance()
        
        comprehensive_report = f"""
ç»¼åˆæ¨¡å‹å¯¹æ¯”æŠ¥å‘Š
================

æ•°æ®é›†ä¿¡æ¯:
- è®­ç»ƒé›†å¤§å°: {len(test_labels)}
- æµ‹è¯•é›†å¤§å°: {len(test_labels)}
- å®ä½“ç±»å‹: 4ç§ (PER, ORG, LOC, GPE)

æ¨¡å‹æ€§èƒ½æ’å:
"""
        
        # æŒ‰F1åˆ†æ•°æ’åº
        sorted_models = sorted(zip(model_names, f1_scores, pred_times), 
                              key=lambda x: x[1], reverse=True)
        
        for i, (model_name, f1, pred_time) in enumerate(sorted_models, 1):
            comprehensive_report += f"{i}. {model_name.upper()}: F1={f1:.4f}, æ—¶é—´={pred_time:.2f}s\n"
        
        comprehensive_report += f"""

æ¨¡å‹åˆ†ç±»åˆ†æ:

ä¼ ç»Ÿæœºå™¨å­¦ä¹ æ¨¡å‹:
- HMM: åŸºäºç»Ÿè®¡çš„åºåˆ—æ ‡æ³¨ï¼Œè®­ç»ƒå¿«é€Ÿï¼Œä½†ç‰¹å¾è¡¨è¾¾èƒ½åŠ›æœ‰é™
- CRF: è€ƒè™‘æ ‡ç­¾é—´ä¾èµ–å…³ç³»ï¼Œç‰¹å¾å·¥ç¨‹ä¸°å¯Œï¼Œæ€§èƒ½è¾ƒå¥½

æŠ€æœ¯ç‰¹ç‚¹å¯¹æ¯”:
| æ¨¡å‹ç±»å‹ | è®­ç»ƒé€Ÿåº¦ | é¢„æµ‹é€Ÿåº¦ | ç‰¹å¾è¡¨è¾¾ | è¯­ä¹‰ç†è§£ | è®¡ç®—èµ„æº |
|---------|---------|---------|---------|---------|---------|
| ä¼ ç»ŸML   | å¿«      | å¿«      | æœ‰é™     | å¼±      | ä½      |

ç»“è®º:
- æœ€ä½³æ€§èƒ½: {sorted_models[0][0].upper()} (F1: {sorted_models[0][1]:.4f})
- æœ€å¿«é€Ÿåº¦: {min(zip(model_names, pred_times), key=lambda x: x[1])[0].upper()}

åº”ç”¨å»ºè®®:
1. èµ„æºå—é™åœºæ™¯: é€‰æ‹©HMM
2. è¿½æ±‚æ›´å¥½æ€§èƒ½: é€‰æ‹©CRF
3. å®æ—¶åº”ç”¨: é€‰æ‹©HMM
4. ç¦»çº¿æ‰¹å¤„ç†: é€‰æ‹©CRF
"""
        
        with open('comprehensive_results/comprehensive_report.txt', 'w', encoding='utf-8') as f:
            f.write(comprehensive_report)
        
        # ä¿å­˜è¯¦ç»†ç»“æœåˆ°JSON
        detailed_results = {
            'model_performance': {
                name: {
                    'f1_score': float(score),
                    'prediction_time': float(time)
                } for name, score, time in zip(model_names, f1_scores, pred_times)
            },
            'best_model': sorted_models[0][0],
            'best_f1_score': float(sorted_models[0][1]),
            'fastest_model': min(zip(model_names, pred_times), key=lambda x: x[1])[0]
        }
        
        with open('comprehensive_results/detailed_results.json', 'w', encoding='utf-8') as f:
            json.dump(detailed_results, f, ensure_ascii=False, indent=2)
        
        print("ç»¼åˆç»“æœå·²ä¿å­˜åˆ° comprehensive_results/ ç›®å½•")
    
    def run_comprehensive_comparison(self, data_file='data/ccfbdci.jsonl'):
        """è¿è¡Œç»¼åˆæ¨¡å‹å¯¹æ¯”"""
        print("ğŸš€ å¼€å§‹ç»¼åˆæ¨¡å‹å¯¹æ¯”")
        print("åŒ…å«æ¨¡å‹: HMM, CRF")
        
        # 1. æ•°æ®å‡†å¤‡
        train_seqs, test_seqs, train_labels, test_labels = self.load_and_prepare_data(data_file)
        
        # 2. è®­ç»ƒæ¨¡å‹
        training_times = self.train_models(train_seqs, train_labels)
        
        # 3. è¯„ä¼°æ¨¡å‹
        evaluation_results = self.evaluate_models(test_seqs, test_labels)
        
        # 4. æ€§èƒ½å¯¹æ¯”
        self.compare_performance()
        
        # 5. åˆ›å»ºå¯è§†åŒ–
        self.create_comprehensive_visualizations(test_labels)
        
        # 6. ä¿å­˜ç»“æœ
        self.save_comprehensive_results(test_labels)
        
        print("\nğŸ‰ ç»¼åˆæ¨¡å‹å¯¹æ¯”å®Œæˆ!")
        print("ç»“æœæ–‡ä»¶ä¿å­˜åœ¨ comprehensive_results/ ç›®å½•")

def main():
    """ä¸»å‡½æ•°"""
    comparison = ComprehensiveModelComparison()
    comparison.run_comprehensive_comparison()

if __name__ == "__main__":
    main() 